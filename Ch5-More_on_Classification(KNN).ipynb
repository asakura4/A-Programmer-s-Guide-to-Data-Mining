{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\Howard\\\\Desktop\\\\Learnbyself\\\\python\\\\A Programmer's Guide to Data Mining\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide to buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data into 10 buckets\n",
    "import random\n",
    "\n",
    "def buckets(filename, bucketName, separator, classColumn):\n",
    "    \"\"\"the original data is in the file named filename\n",
    "    bucketName is the prefix for all the bucket names\n",
    "    separator is the character that divides the columns\n",
    "    (for ex., a tab or comma and classColumn is the column\n",
    "    that indicates the class\"\"\"\n",
    "\n",
    "    # put the data in 10 buckets\n",
    "    numberOfBuckets = 10\n",
    "    data = {}\n",
    "    # first read in the data and divide by category\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if separator != '\\t':\n",
    "            line = line.replace(separator, '\\t')\n",
    "        # first get the category\n",
    "        category = line.split()[classColumn]\n",
    "        data.setdefault(category, [])\n",
    "        data[category].append(line)\n",
    "    # initialize the buckets\n",
    "    buckets = []\n",
    "    for i in range(numberOfBuckets):\n",
    "        buckets.append([])       \n",
    "    # now for each category put the data into the buckets\n",
    "    for k in data.keys():\n",
    "        #randomize order of instances for each class\n",
    "        random.shuffle(data[k])\n",
    "        bNum = 0\n",
    "        # divide into buckets\n",
    "        for item in data[k]:\n",
    "            buckets[bNum].append(item)\n",
    "            bNum = (bNum + 1) % numberOfBuckets\n",
    "\n",
    "    # write to file\n",
    "    for bNum in range(numberOfBuckets):\n",
    "        f = open(\"%s-%02i\" % (bucketName, bNum + 1), 'w')\n",
    "        for item in buckets[bNum]:\n",
    "            f.write(item)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pimaSmall.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c220aa4b9728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example of how to use this code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbuckets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pimaSmall.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pimaSmall'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-0a1c86587fb5>\u001b[0m in \u001b[0;36mbuckets\u001b[1;34m(filename, bucketName, separator, classColumn)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# first read in the data and divide by category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pimaSmall.txt'"
     ]
    }
   ],
   "source": [
    "# example of how to use this code          \n",
    "buckets(\"pimaSmall.txt\", 'pimaSmall',',',8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement 10-folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, bucketPrefix, testBucketNumber, dataFormat):\n",
    "\n",
    "        \"\"\" a classifier will be built from files with the bucketPrefix\n",
    "        excluding the file with textBucketNumber. dataFormat is a string that\n",
    "        describes how to interpret each line of the data files. For example,\n",
    "        for the mpg data the format is:\n",
    "\n",
    "        \"class\tnum\tnum\tnum\tnum\tnum\tcomment\"\n",
    "        \"\"\"\n",
    "   \n",
    "        self.medianAndDeviation = []\n",
    "        \n",
    "        # reading the data in from the file\n",
    " \n",
    "        self.format = dataFormat.strip().split('\\t')\n",
    "        self.data = []\n",
    "        # for each of the buckets numbered 1 through 10:\n",
    "        for i in range(1, 11):\n",
    "            # if it is not the bucket we should ignore, read in the data\n",
    "            if i != testBucketNumber:\n",
    "                filename = \"%s-%02i\" % (bucketPrefix, i)\n",
    "                f = open(filename)\n",
    "                lines = f.readlines()\n",
    "                f.close()\n",
    "                for line in lines:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    ignore = []\n",
    "                    vector = []\n",
    "                    for j in range(len(fields)):\n",
    "                        \n",
    "                        if self.format[j] == 'num':\n",
    "                            vector.append(float(fields[j]))\n",
    "                        elif self.format[j] == 'comment':\n",
    "                            ignore.append(fields[j])\n",
    "                        elif self.format[j] == 'class':\n",
    "                            classification = fields[j]\n",
    "                    self.data.append((classification, vector, ignore))\n",
    "        self.rawData = copy.deepcopy(self.data)\n",
    "        # get length of instance vector\n",
    "        self.vlen = len(self.data[0][1])\n",
    "        # now normalize the data\n",
    "        for i in range(self.vlen):\n",
    "            self.normalizeColumn(i)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    ##################################################\n",
    "    ###\n",
    "    ###  CODE TO COMPUTE THE MODIFIED STANDARD SCORE\n",
    "\n",
    "    def getMedian(self, alist):\n",
    "        \"\"\"return median of alist\"\"\"\n",
    "        if alist == []:\n",
    "            return []\n",
    "        blist = sorted(alist)\n",
    "        length = len(alist)\n",
    "        if length % 2 == 1:\n",
    "            # length of list is odd so return middle element\n",
    "            return blist[int(((length + 1) / 2) -  1)]\n",
    "        else:\n",
    "            # length of list is even so compute midpoint\n",
    "            v1 = blist[int(length / 2)]\n",
    "            v2 =blist[(int(length / 2) - 1)]\n",
    "            return (v1 + v2) / 2.0\n",
    "        \n",
    "\n",
    "    def getAbsoluteStandardDeviation(self, alist, median):\n",
    "        \"\"\"given alist and median return absolute standard deviation\"\"\"\n",
    "        sum = 0\n",
    "        for item in alist:\n",
    "            sum += abs(item - median)\n",
    "        return sum / len(alist)\n",
    "\n",
    "\n",
    "    def normalizeColumn(self, columnNumber):\n",
    "       \"\"\"given a column number, normalize that column in self.data\"\"\"\n",
    "       # first extract values to list\n",
    "       col = [v[1][columnNumber] for v in self.data]\n",
    "       median = self.getMedian(col)\n",
    "       asd = self.getAbsoluteStandardDeviation(col, median)\n",
    "       #print(\"Median: %f   ASD = %f\" % (median, asd))\n",
    "       self.medianAndDeviation.append((median, asd))\n",
    "       for v in self.data:\n",
    "           v[1][columnNumber] = (v[1][columnNumber] - median) / asd\n",
    "\n",
    "\n",
    "    def normalizeVector(self, v):\n",
    "        \"\"\"We have stored the median and asd for each column.\n",
    "        We now use them to normalize vector v\"\"\"\n",
    "        vector = list(v)\n",
    "        for i in range(len(vector)):\n",
    "            (median, asd) = self.medianAndDeviation[i]\n",
    "            vector[i] = (vector[i] - median) / asd\n",
    "        return vector\n",
    "    ###\n",
    "    ### END NORMALIZATION\n",
    "    ##################################################\n",
    "\n",
    "    def testBucket(self, bucketPrefix, bucketNumber):\n",
    "        \"\"\"Evaluate the classifier with data from the file\n",
    "        bucketPrefix-bucketNumber\"\"\"\n",
    "        \n",
    "        filename = \"%s-%02i\" % (bucketPrefix, bucketNumber)\n",
    "        f = open(filename)\n",
    "        lines = f.readlines()\n",
    "        totals = {}\n",
    "        f.close()\n",
    "        for line in lines:\n",
    "            data = line.strip().split('\\t')\n",
    "            vector = []\n",
    "            classInColumn = -1\n",
    "            for i in range(len(self.format)):\n",
    "                  if self.format[i] == 'num':\n",
    "                      vector.append(float(data[i]))\n",
    "                  elif self.format[i] == 'class':\n",
    "                      classInColumn = i\n",
    "            theRealClass = data[classInColumn]\n",
    "            classifiedAs = self.classify(vector)\n",
    "            totals.setdefault(theRealClass, {})\n",
    "            totals[theRealClass].setdefault(classifiedAs, 0)\n",
    "            totals[theRealClass][classifiedAs] += 1\n",
    "        return totals\n",
    "\n",
    "\n",
    "\n",
    "    def manhattan(self, vector1, vector2):\n",
    "        \"\"\"Computes the Manhattan distance.\"\"\"\n",
    "        return sum(map(lambda v1, v2: abs(v1 - v2), vector1, vector2))\n",
    "\n",
    "\n",
    "    def nearestNeighbor(self, itemVector):\n",
    "        \"\"\"return nearest neighbor to itemVector\"\"\"\n",
    "        return min([ (self.manhattan(itemVector, item[1]), item)\n",
    "                     for item in self.data])\n",
    "    \n",
    "    def classify(self, itemVector):\n",
    "        \"\"\"Return class we think item Vector is in\"\"\"\n",
    "        return(self.nearestNeighbor(self.normalizeVector(itemVector))[1][0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tenfold(bucketPrefix, dataFormat):\n",
    "    results = {}\n",
    "    for i in range(1, 11):\n",
    "        c = Classifier(bucketPrefix, i, dataFormat)\n",
    "        t = c.testBucket(bucketPrefix, i)\n",
    "        for (key, value) in t.items():\n",
    "            results.setdefault(key, {})\n",
    "            for (ckey, cvalue) in value.items():\n",
    "                results[key].setdefault(ckey, 0)\n",
    "                results[key][ckey] += cvalue\n",
    "                \n",
    "    # now print results\n",
    "    categories = list(results.keys())\n",
    "    categories.sort()\n",
    "    print(   \"\\n       Classified as: \")\n",
    "    header =    \"        \"\n",
    "    subheader = \"      +\"\n",
    "    for category in categories:\n",
    "        header += category + \"   \"\n",
    "        subheader += \"----+\"\n",
    "    print (header)\n",
    "    print (subheader)\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for category in categories:\n",
    "        row = category + \"    |\"\n",
    "        for c2 in categories:\n",
    "            if c2 in results[category]:\n",
    "                count = results[category][c2]\n",
    "            else:\n",
    "                count = 0\n",
    "            row += \" %2i |\" % count\n",
    "            total += count\n",
    "            if c2 == category:\n",
    "                correct += count\n",
    "        print(row)\n",
    "    print(subheader)\n",
    "    print(\"\\n%5.3f percent correct\" %((correct * 100) / total))\n",
    "    print(\"total of %i instances\" % total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Classified as: \n",
      "        10   15   20   25   30   35   40   45   \n",
      "      +----+----+----+----+----+----+----+----+\n",
      "10    |  5 |  8 |  0 |  0 |  0 |  0 |  0 |  0 |\n",
      "15    |  8 | 63 | 14 |  1 |  0 |  0 |  0 |  0 |\n",
      "20    |  0 | 14 | 67 |  8 |  5 |  1 |  1 |  0 |\n",
      "25    |  0 |  1 | 13 | 35 | 22 |  6 |  1 |  1 |\n",
      "30    |  0 |  1 |  3 | 17 | 21 | 14 |  5 |  2 |\n",
      "35    |  0 |  0 |  2 |  7 | 10 | 13 |  5 |  1 |\n",
      "40    |  0 |  0 |  1 |  0 |  5 |  5 |  0 |  0 |\n",
      "45    |  0 |  0 |  0 |  2 |  1 |  1 |  0 |  2 |\n",
      "      +----+----+----+----+----+----+----+----+\n",
      "\n",
      "52.551 percent correct\n",
      "total of 392 instances\n"
     ]
    }
   ],
   "source": [
    "tenfold(\"C:\\\\Users\\\\Howard\\\\Desktop\\\\Learnbyself\\\\python\\\\A Programmer's Guide to Data Mining\\\\mpgData\\\\mpgData\",\n",
    "        \"class\tnum\tnum\tnum\tnum\tnum\tcomment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  \n",
    "# \n",
    "#  Nearest Neighbor Classifier for Pima dataset\n",
    "#\n",
    "#\n",
    "#  Code file for the book Programmer's Guide to Data Mining\n",
    "#  http://guidetodatamining.com\n",
    "#\n",
    "#  Ron Zacharski\n",
    "#\n",
    "import heapq\n",
    "import random\n",
    "\n",
    "class ClassifierK:\n",
    "    def __init__(self, bucketPrefix, testBucketNumber, dataFormat, k):\n",
    "\n",
    "        \"\"\" a classifier will be built from files with the bucketPrefix\n",
    "        excluding the file with textBucketNumber. dataFormat is a string that\n",
    "        describes how to interpret each line of the data files. For example,\n",
    "        for the mpg data the format is:\n",
    "\n",
    "        \"class\tnum\tnum\tnum\tnum\tnum\tcomment\"\n",
    "        \"\"\"\n",
    "   \n",
    "        self.medianAndDeviation = []\n",
    "        self.k = k\n",
    "        # reading the data in from the file\n",
    " \n",
    "        self.format = dataFormat.strip().split('\\t')\n",
    "        self.data = []\n",
    "        # for each of the buckets numbered 1 through 10:\n",
    "        for i in range(1, 11):\n",
    "            # if it is not the bucket we should ignore, read in the data\n",
    "            if i != testBucketNumber:\n",
    "                filename = \"%s-%02i\" % (bucketPrefix, i)\n",
    "                f = open(filename)\n",
    "                lines = f.readlines()\n",
    "                f.close()\n",
    "                for line in lines[1:]:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    ignore = []\n",
    "                    vector = []\n",
    "                    for i in range(len(fields)):\n",
    "                        if self.format[i] == 'num':\n",
    "                            vector.append(float(fields[i]))\n",
    "                        elif self.format[i] == 'comment':\n",
    "                            ignore.append(fields[i])\n",
    "                        elif self.format[i] == 'class':\n",
    "                            classification = fields[i]\n",
    "                    self.data.append((classification, vector, ignore))\n",
    "        self.rawData = list(self.data)\n",
    "        # get length of instance vector\n",
    "        self.vlen = len(self.data[0][1])\n",
    "        # now normalize the data\n",
    "        for i in range(self.vlen):\n",
    "            self.normalizeColumn(i)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    ##################################################\n",
    "    ###\n",
    "    ###  CODE TO COMPUTE THE MODIFIED STANDARD SCORE\n",
    "\n",
    "    def getMedian(self, alist):\n",
    "        \"\"\"return median of alist\"\"\"\n",
    "        if alist == []:\n",
    "            return []\n",
    "        blist = sorted(alist)\n",
    "        length = len(alist)\n",
    "        if length % 2 == 1:\n",
    "            # length of list is odd so return middle element\n",
    "            return blist[int(((length + 1) / 2) -  1)]\n",
    "        else:\n",
    "            # length of list is even so compute midpoint\n",
    "            v1 = blist[int(length / 2)]\n",
    "            v2 =blist[(int(length / 2) - 1)]\n",
    "            return (v1 + v2) / 2.0\n",
    "        \n",
    "\n",
    "    def getAbsoluteStandardDeviation(self, alist, median):\n",
    "        \"\"\"given alist and median return absolute standard deviation\"\"\"\n",
    "        sum = 0\n",
    "        for item in alist:\n",
    "            sum += abs(item - median)\n",
    "        return sum / len(alist)\n",
    "\n",
    "\n",
    "    def normalizeColumn(self, columnNumber):\n",
    "       \"\"\"given a column number, normalize that column in self.data\"\"\"\n",
    "       # first extract values to list\n",
    "       col = [v[1][columnNumber] for v in self.data]\n",
    "       median = self.getMedian(col)\n",
    "       asd = self.getAbsoluteStandardDeviation(col, median)\n",
    "       #print(\"Median: %f   ASD = %f\" % (median, asd))\n",
    "       self.medianAndDeviation.append((median, asd))\n",
    "       for v in self.data:\n",
    "           v[1][columnNumber] = (v[1][columnNumber] - median) / asd\n",
    "\n",
    "\n",
    "    def normalizeVector(self, v):\n",
    "        \"\"\"We have stored the median and asd for each column.\n",
    "        We now use them to normalize vector v\"\"\"\n",
    "        vector = list(v)\n",
    "        for i in range(len(vector)):\n",
    "            (median, asd) = self.medianAndDeviation[i]\n",
    "            vector[i] = (vector[i] - median) / asd\n",
    "        return vector\n",
    "    ###\n",
    "    ### END NORMALIZATION\n",
    "    ##################################################\n",
    "\n",
    "    def testBucket(self, bucketPrefix, bucketNumber):\n",
    "        \"\"\"Evaluate the classifier with data from the file\n",
    "        bucketPrefix-bucketNumber\"\"\"\n",
    "        \n",
    "        filename = \"%s-%02i\" % (bucketPrefix, bucketNumber)\n",
    "        f = open(filename)\n",
    "        lines = f.readlines()\n",
    "        totals = {}\n",
    "        f.close()\n",
    "        for line in lines:\n",
    "            data = line.strip().split('\\t')\n",
    "            vector = []\n",
    "            classInColumn = -1\n",
    "            for i in range(len(self.format)):\n",
    "                  if self.format[i] == 'num':\n",
    "                      vector.append(float(data[i]))\n",
    "                  elif self.format[i] == 'class':\n",
    "                      classInColumn = i\n",
    "            theRealClass = data[classInColumn]\n",
    "            #print(\"REAL \", theRealClass)\n",
    "            classifiedAs = self.classify(vector)\n",
    "            totals.setdefault(theRealClass, {})\n",
    "            totals[theRealClass].setdefault(classifiedAs, 0)\n",
    "            totals[theRealClass][classifiedAs] += 1\n",
    "        return totals\n",
    "\n",
    "\n",
    "\n",
    "    def manhattan(self, vector1, vector2):\n",
    "        \"\"\"Computes the Manhattan distance.\"\"\"\n",
    "        return sum(map(lambda v1, v2: abs(v1 - v2), vector1, vector2))\n",
    "\n",
    "\n",
    "    def nearestNeighbor(self, itemVector):\n",
    "        \"\"\"return nearest neighbor to itemVector\"\"\"\n",
    "        return min([ (self.manhattan(itemVector, item[1]), item)\n",
    "                     for item in self.data])\n",
    "    \n",
    "    def knn(self, itemVector):\n",
    "        \"\"\"returns the predicted class of itemVector using k\n",
    "        Nearest Neighbors\"\"\"\n",
    "        # changed from min to heapq.nsmallest to get the\n",
    "        # k closest neighbors\n",
    "        neighbors = heapq.nsmallest(self.k, [(self.manhattan(itemVector, item[1]), item) \\\n",
    "                                              for item in self.data ])\n",
    "        # each neighbor gets a vote\n",
    "        results = {}\n",
    "        for neighbor in neighbors:\n",
    "            theClass = neighbor[1][0]\n",
    "            results.setdefault(theClass, 0)\n",
    "            results[theClass] += 1\n",
    "        resultList = sorted([(i[1], i[0]) for i in results.items()], \n",
    "                           reverse = True)\n",
    "        # get all the classes that have the maximum votes\n",
    "        maxVotes = resultList[0][0]\n",
    "        possibleAnswers = [i[1] for i in resultList if i[0] == maxVotes]\n",
    "        # randomly select one of the classes that received the max votes\n",
    "        answer = random.choice(possibleAnswers)\n",
    "        return answer\n",
    "    \n",
    "    def classify(self, itemVector):\n",
    "        \"\"\"Return class we think item Vector is in\"\"\"\n",
    "        # k represents how many nearest neighbors to use\n",
    "        return(self.knn(self.normalizeVector(itemVector)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tenfoldK(bucketPrefix, dataFormat, k):\n",
    "    results = {}\n",
    "    for i in range(1, 11):\n",
    "        c = ClassifierK(bucketPrefix, i, dataFormat, k)\n",
    "        t = c.testBucket(bucketPrefix, i)\n",
    "        for (key, value) in t.items():\n",
    "            results.setdefault(key, {})\n",
    "            for (ckey, cvalue) in value.items():\n",
    "                results[key].setdefault(ckey, 0)\n",
    "                results[key][ckey] += cvalue\n",
    "                \n",
    "    # now print results\n",
    "    categories = list(results.keys())\n",
    "    categories.sort()\n",
    "    print(   \"\\n       Classified as: \")\n",
    "    header =    \"        \"\n",
    "    subheader = \"      +\"\n",
    "    for category in categories:\n",
    "        header += \"% 2s   \" % category\n",
    "        subheader += \"-----+\"\n",
    "    print (header)\n",
    "    print (subheader)\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for category in categories:\n",
    "        row = \" %s    |\" % category \n",
    "        for c2 in categories:\n",
    "            if c2 in results[category]:\n",
    "                count = results[category][c2]\n",
    "            else:\n",
    "                count = 0\n",
    "            row += \" %3i |\" % count\n",
    "            total += count\n",
    "            if c2 == category:\n",
    "                correct += count\n",
    "        print(row)\n",
    "    print(subheader)\n",
    "    print(\"\\n%5.3f percent correct\" %((correct * 100) / total))\n",
    "    print(\"total of %i instances\" % total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMALL DATA SET\n",
      "\n",
      "       Classified as: \n",
      "         0    1   \n",
      "      +-----+-----+\n",
      " 0    |  45 |  14 |\n",
      " 1    |  27 |  14 |\n",
      "      +-----+-----+\n",
      "\n",
      "59.000 percent correct\n",
      "total of 100 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"SMALL DATA SET\")\n",
    "tenfoldK(\"C:\\\\Users\\\\Howard\\\\Desktop\\\\Learnbyself\\\\python\\\\A Programmer's Guide to Data Mining\\\\pimaSmall\\\\pimaSmall\",\n",
    "        \"num\tnum\tnum\tnum\tnum\tnum\tnum\tnum\tclass\", 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
